{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T18:43:38.731044Z",
     "start_time": "2020-12-05T18:43:38.726486Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import decomposition\n",
    "import seaborn as sns; sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
    "import skbio\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import time\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.manifold import TSNE\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import plotly.express as px\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T19:16:56.067840Z",
     "start_time": "2020-12-05T19:16:56.059636Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def dataframe_to_dataset(data_df):\n",
    "    dataframe = df.copy()\n",
    "    labels = dataframe.pop(\"label\")\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    return ds\n",
    "\n",
    "def pred_dataframe_to_dataset(dataframe):\n",
    "    dataframe = dataframe.copy()\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe)))\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T18:14:15.539925Z",
     "start_time": "2020-12-05T18:14:15.536321Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def encode_numerical_feature(feature, name, dataset):\n",
    "    # Create a Normalization layer for our feature\n",
    "    normalizer = Normalization()\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the statistics of the data\n",
    "    normalizer.adapt(feature_ds)\n",
    "\n",
    "    # Normalize the input feature\n",
    "    encoded_feature = normalizer(feature)\n",
    "    return encoded_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T13:59:03.588254Z",
     "start_time": "2020-12-05T13:59:03.178384Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_path = \"../../../data/starting_data/not-filtered-data/\"\n",
    "features_path = os.path.join(data_path,\"OTU_FIX_feature-table-l7.csv\")\n",
    "edited_metadta = f\"{data_path}/../edited_metadata.tsv\"\n",
    "data_df = pd.read_csv(features_path,sep='\\t')\n",
    "    \n",
    "# if drop_unassigned:\n",
    "data_df = data_df.loc[~data_df[\"OTU ID\"]\n",
    "                          .str.contains(\"Unassigned\")].reset_index(drop=True)\n",
    "\n",
    "#transform on groupby perform the action but keep all rows and just duplicate the values to match original DF\n",
    "data_df_summed = data_df.groupby(['OTU ID']).transform(\"sum\")\n",
    "# gropuby removes the column on wihch it was used. So I'm copying it from the original data_df \n",
    "data_df_summed['OTU ID'] = data_df['OTU ID']\n",
    "# reorder the columns such the \"OTU ID\" is first column\n",
    "data_df_summed = data_df_summed[['OTU ID']+[c for c in data_df_summed if c not in ['OTU ID']]]\n",
    "\n",
    "# Drop the duplicates rows now with same values thanks to the transform method\n",
    "data_df_summed.drop_duplicates(inplace=True)\n",
    "\n",
    "meta_df = pd.read_csv(edited_metadta,sep='\\t').rename(columns={\"sampleID\":\"sample_name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T14:32:02.523062Z",
     "start_time": "2020-12-05T14:32:02.507375Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# replace the \"sample_time\" with numbers corresponding the time \n",
    "sample_time_enum = {sample_time:i for i,sample_time in enumerate(meta_df.sort_values('visit_age_mo').sample_time.unique().tolist())}\n",
    "sample_time_enum['sick']=len(sample_time_enum)+5\n",
    "\n",
    "data_df_indexed = data_df.set_index(\"OTU ID\",drop=True)\n",
    "meta_df = meta_df.assign(sample_time_enum = meta_df.sample_time)\n",
    "meta_df.replace({\"sample_time_enum\":sample_time_enum},inplace=True)\n",
    "meta_df = meta_df[['sample_time_enum']+[c for c in meta_df if c not in ['sample_time_enum']]]\n",
    "meta_df.head()\n",
    "\n",
    "data_idx = meta_df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T14:21:53.300070Z",
     "start_time": "2020-12-05T14:21:53.271045Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "merged_df = meta_df.merge(data_df_indexed.T,right_index=True,left_on=['sample_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T14:04:10.289429Z",
     "start_time": "2020-12-05T14:04:10.285247Z"
    }
   },
   "source": [
    "## Creatin the Training Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T14:23:19.555282Z",
     "start_time": "2020-12-05T14:23:19.532440Z"
    }
   },
   "source": [
    "Expects to have a `merged_df` dataframe where the rows are subjects and columns are both metadata and bacterai names.\n",
    "\n",
    "Expects `data_idx` variable to be the first idx of the data (bacteria) in the `merged_df`. before that are the meta columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T14:30:56.696086Z",
     "start_time": "2020-12-05T14:30:56.690025Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.9\n",
       "1       0.1\n",
       "2       0.2\n",
       "3       0.7\n",
       "4       1.1\n",
       "       ... \n",
       "1083    3.4\n",
       "1084    3.9\n",
       "1085    6.3\n",
       "1086    6.3\n",
       "1087    4.3\n",
       "Name: visit_age_mo, Length: 1088, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_df.visit_age_mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T18:10:24.778264Z",
     "start_time": "2020-12-05T18:10:24.771041Z"
    }
   },
   "outputs": [],
   "source": [
    "control_merged_data = merged_df[merged_df.symptoms == \"Control\"]\n",
    "data_df = control_merged_data.iloc[:,data_idx:]\n",
    "data_df['label'] = control_merged_data.visit_age_mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T19:26:11.575216Z",
     "start_time": "2020-12-05T19:26:11.568985Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df = data_df.sample(frac=0.2,random_state=666)\n",
    "train_df = data_df.drop(test_df.index)\n",
    "\n",
    "X_train = train_df.values.astype(np.float32)\n",
    "Y_train = train_df.label.values.astype(np.float32)\n",
    "Y_train = Y_train[...,np.newaxis]\n",
    "# Y_data = control_merged_data.visit_age_mo.values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T19:17:01.965365Z",
     "start_time": "2020-12-05T19:17:01.940036Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-fd0a9eade77c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe_to_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe_to_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mval_ds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-96-ac7e17f31365>\u001b[0m in \u001b[0;36mdataframe_to_dataset\u001b[0;34m(data_df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdataframe_to_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "train_ds = dataframe_to_dataset(train_df)\n",
    "test_df = dataframe_to_dataset(test_df)\n",
    "\n",
    "train_ds = train_ds.batch(32)\n",
    "val_ds = val_ds.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T18:55:15.435472Z",
     "start_time": "2020-12-05T18:55:15.430950Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomModel(Model):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomModel, self).__init__(**kwargs)\n",
    "        self.dense1 = Dense(128, activation='relu',name=\"Dense_1\" )\n",
    "        self.dropout1 = Dropout(0.2)\n",
    "        self.dense2 = Dense(64, activation='relu',name=\"Dense_2\")\n",
    "        self.dropout2 = Dropout(0.2)\n",
    "        self.dense3 = Dense(32, activation='relu',name=\"Dense_3\")\n",
    "        self.predict = Dense(1, activation='sigmoid')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        return self.dense3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T18:56:17.704364Z",
     "start_time": "2020-12-05T18:56:17.678934Z"
    }
   },
   "outputs": [],
   "source": [
    "model= CustomModel(name='customModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T19:06:10.119698Z",
     "start_time": "2020-12-05T19:06:10.098252Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T19:25:02.130965Z",
     "start_time": "2020-12-05T19:25:02.128608Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T19:25:03.765894Z",
     "start_time": "2020-12-05T19:25:03.720271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-d8c99065c449>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(X_train,Y_train,\n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     epochs= 10)\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# history = my_custom_model.fit(train_ds, y_train,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#                     batch_size= 64,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py37-tf2/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py37-tf2/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py37-tf2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py37-tf2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,Y_train,\n",
    "                    batch_size=32,\n",
    "                    epochs= 10)\n",
    "# history = my_custom_model.fit(train_ds, y_train,\n",
    "#                     batch_size= 64,\n",
    "#                     epochs= 10,\n",
    "#                     validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(df,data_col_pos,elements_per_sample = 3):\n",
    "    \"\"\"\n",
    "    elements_per_sample: how many examples will appear together for the training\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    train_data = list()\n",
    "    is_sick_data = list()\n",
    "    meta_data = list()\n",
    "    records_ids = df.record_id.unique().tolist()\n",
    "\n",
    "    #build the data for each record\n",
    "    for rid in records_ids:\n",
    "        subject_df = df[df.record_id == rid]\n",
    "\n",
    "        # Transform the df to numpy and take only the columns for the data (and not the metadata)\n",
    "        full_data_arr = subject_df.iloc[:,data_col_pos:].to_numpy()\n",
    "        sample_meta_data = subject_df.iloc[:,data_col_pos:].to_numpy()\n",
    "        n = subject_df.shape[0]\n",
    "\n",
    "        # get the idxs ot the elements to take for each training sample\n",
    "        sets_idxs = list(combinations(np.arange(0,n),elements_per_sample))\n",
    "\n",
    "        subject_train_samples = np.take(full_data_arr,sets_idxs,axis=0)\n",
    "\n",
    "        # decide if this each sample is sick or not if at least one of the samples is tagged as sick\n",
    "        # TODO - using AP case is because the symptoms might have probmels -  sample 29 is assigned as \"AP Case\" but        doesn't have \"Symptomatic\" in the symptoms\n",
    "        is_sick = np.array([any(subject_df.iloc[list(idx)].case_id == \"AP Case\") for idx in sets_idxs])\n",
    "\n",
    "        train_data.append(subject_train_samples)\n",
    "        is_sick_data.append(is_sick)\n",
    "        \n",
    "#         if (subject_df.case_id == 'AP Case').any():\n",
    "#             symptoms = subject_df.symptoms.unique()\n",
    "#             if len(symptoms) == 1:\n",
    "#                 print(\"#################################\")\n",
    "#                 print(f\"Printing symptoms for id {rid}\")\n",
    "#                 print(subject_df.symptoms.unique())\n",
    "    return train_data, is_sick_data"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
